#!/bin/bash

# export CUDA_VISIBLE_DEVICES=4,5,6,7
# export CUDA_LAUNCH_BLOCKING=1
export HF_HOME='/home/xuxinchen/data/hfhub'
export HF_ENDPOINT='https://hf-mirror.com'
export TIMEOUT_NCCL_MINUTES=120
export ASCEND_LAUNCH_BLOCKING=1
# export ASCEND_RT_VISIBLE_DEVICES=4,5,6,7
export ASCEND_RT_VISIBLE_DEVICES=0,1,2,3

run_experiment() {
    local args="$@"
    local timestamp=$(date +"%Y%m%d-%H%M%S")
    torchrun --nnodes=1 --nproc_per_node=4 --master_port=29501 train.py \
        --log_dir "logs/sdiloco/$timestamp" \
        $args
}


### LLAMA150M C4EN
# DiLoCo H=50
run_experiment --dataset_name "c4en" --model_name "llama1b" \
    --sync_interval 100 --use_nesterov \
    --eval_interval 100 \
    --use_amp --amp_type 'bf16' --total_steps 44000 \
    --checkpoint_interval 100 --checkpoint_dir 'ckpts/sdiloco_1b' \
    --max_checkpoints 5 \
    --delay_steps 5 \
    --num_shards 8 \
    --N 10 \
    --algorithm "streaming" \
    --batch_size 4 \
    --effective_batch_size 256 --resume 
    # \
    # --resume
    # --algorithm "streaming" \ 
    # --algorithm "diloco" \
    # --algorithm "dc" \ 
    # --simulated_comp_time 0.3 \

### BERT SST2
# DP
# run_experiment --sync_interval 1 --outer_lr 1.0 --use_nesterov

# DiLoCo H=50
# run_experiment --sync_interval 50 --outer_lr 0.7 --use_nesterov